{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours TAL – Labo 3 : analyse syntaxique\n",
    "Nathan Gonzalez Montes et Vincent Guidoux\n",
    "\n",
    "## Exercice 1\n",
    "\n",
    "### Manipulations\n",
    "D'abord, on réalise les tests de performance avec le fichier obtenu `UD_French.gz` sur les fichier `fr-ud-test.conllu3` et `fr-ud-dev.conllu3` et on vérifie les résultats. Après avoir testé, on réalise notre entraînement avec le fichier `fr-ud-train.conllu3` en reprenant comme model le même fichier qu'avant, en modifiant le nom pour avoir notre propre fichier entraîné (on a mis `UD_French_train.gz` comme nom, ). Après la réalisation de notre entraînement, on relance les tests de performance avec le fichier entraîné pour voir la différence dans le résultats (améliorés grâce à notre entraînement).\n",
    "\n",
    "### Test sans notre entraînement\n",
    "#### Test file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-test.conllu3 -model data/UD_French.gz`\n",
    "\n",
    "On obtient les résultats suivants:\n",
    "\n",
    "**OOV Words:** 608 / 10020 = 6.07%\n",
    "\n",
    "**UAS** = 55.0699\n",
    "\n",
    "**LAS** = 41.1577\n",
    "\n",
    "DependencyParser parsed 10020 words in 416 sentences in 4.7s at 2129.2 w/s, 88.4 sent/s.\n",
    "\n",
    "![image](../Screenshot/test_before_train.PNG)\n",
    "\n",
    "#### Dev file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-dev.conllu3 -model data/UD_French.gz`\n",
    "\n",
    "On obtient les résultats suivants:\n",
    "\n",
    "**OOV Words:** 2716 / 35771 = 7.59%\n",
    "\n",
    "**UAS** = 57.2195\n",
    "\n",
    "**LAS** = 43.6722\n",
    "\n",
    "DependencyParser parsed 10020 words in 1478 sentences in 18.1s at 1972.4 w/s, 81.5 sent/s.\n",
    "\n",
    "![image](../Screenshot/dev_before_train.PNG)\n",
    "\n",
    "### Test avec notre entraînement\n",
    "#### Entraînement\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -trainFile data/fr-ud-train.conllu3 -model data/UD_French_train.gz -wordCutOff 3 -trainingThreads 6 -maxIter 5000`\n",
    "\n",
    "`-wordCutOff 3` - Pour traiter seulement les mots apparaissant plus de 3 fois, ce qui évite le problème des nombres \"uniques\" avec un espace.\n",
    "\n",
    "`‑trainingThreads 8` - Pour utiliser pleinement son processeur, indiquer le maximum selon le model de la documentation (On our 16-core test machines: a batch size of 10,000 runs fastest with around 6 threads; a batch size of 100,000 runs best with around 10 threads)\n",
    "\n",
    "`‑maxIter 5000` - Pour arrêter l’entraînement après 5'000 itérations, de base il fait 20'000 itérations.\n",
    "\n",
    "Un exemple des itérations de l'entraînement:\n",
    "![image](../Screenshot/iterations.PNG)\n",
    "\n",
    "#### Test file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-test.conllu3 -model data/UD_French_train.gz`\n",
    "\n",
    "On obtient les résultats suivants:\n",
    "\n",
    "**OOV Words:** 1100 / 10020 = 10.98%\n",
    "\n",
    "**UAS** = 77.8842\n",
    "\n",
    "**LAS** = 71.4571\n",
    "\n",
    "DependencyParser parsed 10020 words in 416 sentences in 1.4s at 7308.5 w/s, 303.4 sent/s.\n",
    "\n",
    "![image](../Screenshot/test_after_train.PNG)\n",
    "\n",
    "#### Dev file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-dev.conllu3 -model data/UD_French_train.gz`\n",
    "\n",
    "On obtient les résultats suivants:\n",
    "\n",
    "**OOV Words:** 4724 / 35771 = 13.21%\n",
    "\n",
    "**UAS** = 80.5094\n",
    "\n",
    "**LAS** = 74.5241\n",
    "\n",
    "DependencyParser parsed 35771 words in 1478 sentences in 2.7s at 13165.6 w/s, 544.0 sent/s.\n",
    "\n",
    "![image](../Screenshot/dev_after_train.PNG)\n",
    "\n",
    "### Question\n",
    "Quel est le score du modèle fourni, et quel est le score du modèle que vous avez entraîné ?\n",
    "\n",
    "Pour le modèle fourni, on obtient pour **UAS** (qui ne considère pas la relation sémantique) un score de **55.0699%** pour le fichier `test` et **57.2195%** pour le fichier `dev`. Tandis que pour **LAS** (qui considère la relation sémantique), on obtient un score de **41.1577%** pour le fichier `test` et **43.6722%** pour le fichier `dev`.\n",
    "\n",
    "Quant au modèle que nous avons entraîné, pour **UAS** on obtient **77.8842%** pour le fichier `test` et **80.5094%** pour le fichier `dev`. Par rapport à **LAS**, on obtient un **74.5241%** comme score pour le fichier `test` et **71.4571%** pour le fichier `dev`, après avoir fait un entraînement de 5000 itérations\n",
    "\n",
    "## Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "from collections import Counter\n",
    "from nltk.parse import (\n",
    "    DependencyGraph,\n",
    "    ProjectiveDependencyParser,\n",
    "    NonprojectiveDependencyParser,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vous devez lire le(s) fichier(s) UD phrase par phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/fr-ud-test.conllu3'  \n",
    "dependency_graphs = []\n",
    "\n",
    "try:  \n",
    "    fp = open(filepath, 'r',encoding=\"utf-8\")\n",
    "    raw_sentences = fp.read().split('\\n\\n')\n",
    "finally:  \n",
    "    fp.close()\n",
    "\n",
    "nbr_raw_sents = len(raw_sentences) - 1 ## the file end with 4 '\\n'\n",
    "    \n",
    "for i in range(nbr_raw_sents):\n",
    "    try:\n",
    "        dependency_graphs.append(DependencyGraph(raw_sentences[i], top_relation_label='root'))\n",
    "    except:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il faut ensuite extraire les triplets ayant une relation ‘nsubj’ (entre sujet et verbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsubj_triples = []\n",
    "\n",
    "for dependency_graph in dependency_graphs:\n",
    "    for head, rel, dep in dependency_graph.triples():\n",
    "        if rel == 'nsubj':\n",
    "            nsubj_triples.append((head, dep))\n",
    "            \n",
    "occurences = Counter(nsubj_triples)\n",
    "\n",
    "# sorted_by_second = sorted(list(occurences.items()), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quels sont les 10 triplets les plus fréquents dans tout le corpus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Il a\" occure 7 fois \n",
      "\"on peut\" occure 4 fois \n",
      "\"il a\" occure 4 fois \n",
      "\"c' est\" occure 3 fois \n",
      "\"il contrôle\" occure 3 fois \n",
      "\"il faut\" occure 3 fois \n",
      "\"qui font\" occure 2 fois \n",
      "\"elle guette\" occure 2 fois \n",
      "\"vous avez\" occure 2 fois \n",
      "\"je vois\" occure 2 fois \n"
     ]
    }
   ],
   "source": [
    "# sorted_by_second[:10]\n",
    "occurences.most_common(10)\n",
    "\n",
    "for (triplet, nbr_occurences) in occurences.most_common(10):\n",
    "    print('\"{pron} {verb}\" occure {nbr_occurences} fois '.format(\n",
    "        pron=triplet[1][0],\n",
    "        verb=triplet[0][0],\n",
    "        nbr_occurences=nbr_occurences,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "\n",
    "### Démarrer le serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !java -mx4g -cp \"stanford-corenlp-3.9.2.jar;stanford-corenlp-3.9.2-models-french.jar\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                ROOT                                         \n",
      "                                 |                                            \n",
      "                                SENT                                         \n",
      "                                 |                                            \n",
      "                                 NP                                          \n",
      "  _______________________________|___________________________________         \n",
      " |    |                PP                              |             |       \n",
      " |    |      __________|_____                          |             |        \n",
      " |    |     |                NP                        |             |       \n",
      " |    |     |     ___________|___                      |             |        \n",
      " |    |     |    |               PP                    |            AdP      \n",
      " |    |     |    |      _________|____                 |         ____|____    \n",
      " |    |     |    |     |              NP               |      MWADV       |  \n",
      " |    |     |    |     |          ____|_______         |    ____|____     |   \n",
      "PRON VERB  VERB CONJ  VERB      DET          NOUN     ADP DET       ADV  ADV \n",
      " |    |     |    |     |         |            |        |   |         |    |   \n",
      " J'  aime faire  et  rendre     mes      laboratoires  à   le       plus vite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next(parser.raw_parse('J\\'aime faire et rendre mes laboratoires au plus vite')).pretty_print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'separator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4fa86dcd6fb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                                   \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                                   \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                                   tagset='universal')\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'separator'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "#source : http://techstuffbrazil.blogspot.com/2017/03/quick-tutorial-to-nltk-corpus-reader-of.html\n",
    "\n",
    "root = './data/'\n",
    "test = 'fr-ud-test.conllu3'\n",
    "COLUMN_TYPES = ('ignore', \n",
    "                'words', \n",
    "                'ignore', \n",
    "                'pos', \n",
    "                'ignore', \n",
    "                'ignore', \n",
    "                'tree', \n",
    "                'ignore', \n",
    "                'ignore', \n",
    "                'ignore')\n",
    "\n",
    "testFile  = ConllCorpusReader(root=root, \n",
    "                                  fileids=test, \n",
    "                                  columntypes=COLUMN_TYPES, \n",
    "                                  encoding='utf8', \n",
    "                                  separator=\"\\t\", \n",
    "                                  tagset='universal')\n",
    "\n",
    "test_sentences = testFile.sents()\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    current_sentence = ''\n",
    "    for word in sentence:\n",
    "        current_sentence = current_sentence + word + ' '\n",
    "    sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-edd838c199d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "for sentence in sentences[:10]:\n",
    "    try:\n",
    "        trees.append(parser.raw_parse(sentence))\n",
    "    except:\n",
    "        print(\"Error\")   \n",
    "print('fini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tree in trees:\n",
    "    for what in tree:\n",
    "        print(what)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
