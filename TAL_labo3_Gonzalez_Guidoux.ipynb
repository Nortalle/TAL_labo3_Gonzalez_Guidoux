{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours TAL – Labo 3 : analyse syntaxique\n",
    "Nathan Gonzalez Montes et Vincent Guidoux\n",
    "\n",
    "## Exercice 1\n",
    "\n",
    "### Manipulations\n",
    "D'abord, on réalise les tests de performance avec le fichier obtenu `UD_French.gz` sur les fichier `fr-ud-test.conllu3` et `fr-ud-dev.conllu3` et on vérifie les résultats. Après avoir testé, on réalise un entraînement avec le fichier `fr-ud-train.conllu3` en reprenant comme model le même fichier qu'avant, en modifiant le nom pour avoir notre propre fichier entraîné (on a mis `UD_French_train.gz` comme nom, ). Après la réalisation de l'entraînement, on relance les tests de performance avec le fichier entraîné pour voir la différence dans le résultats (améliorés grâce à l'entraînement).\n",
    "\n",
    "### Test sans entraînement\n",
    "#### Test file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-test.conllu3 -model data/UD_French.gz`\n",
    "\n",
    "Loading depparse model: data/UD_French.gz ...\n",
    "###################\n",
    "#Transitions: 81\n",
    "#Labels: 40\n",
    "\n",
    "ROOTLABEL: root\n",
    "\n",
    "PreComputed 99996, Elapsed Time: 15.279 (s)\n",
    "\n",
    "Initializing dependency parser ... done [16.6 sec].\n",
    "\n",
    "Test File: data/fr-ud-test.conllu3\n",
    "\n",
    "OOV Words: 608 / 10020 = 6.07%\n",
    "\n",
    "UAS = 55.0699\n",
    "\n",
    "LAS = 41.1577\n",
    "\n",
    "DependencyParser parsed 10020 words in 416 sentences in 4.7s at 2129.2 w/s, 88.4 sent/s.\n",
    "\n",
    "#### Dev file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-dev.conllu3 -model data/UD_French.gz`\n",
    "\n",
    "Loading depparse model: data/UD_French.gz ...\n",
    "###################\n",
    "\n",
    "#Transitions: 81\n",
    "\n",
    "#Labels: 40\n",
    "\n",
    "ROOTLABEL: root\n",
    "\n",
    "PreComputed 99996, Elapsed Time: 16.44 (s)\n",
    "\n",
    "Initializing dependency parser ... done [17.8 sec].\n",
    "\n",
    "Test File: data/fr-ud-dev.conllu3\n",
    "\n",
    "OOV Words: 2716 / 35771 = 7.59%\n",
    "\n",
    "UAS = 57.2195\n",
    "\n",
    "LAS = 43.6722\n",
    "\n",
    "DependencyParser parsed 10020 words in 1478 sentences in 18.1s at 1972.4 w/s, 81.5 sent/s.\n",
    "\n",
    "\n",
    "### Test avec entraînement\n",
    "#### Entraînement\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -trainFile data/fr-ud-train.conllu3 -model data/UD_French_train.gz -wordCutOff 3 -trainingThreads 6 -maxIter 5000`\n",
    "\n",
    "`-wordCutOff 3` - Pour traiter seulement les mots apparaissant plus de 3 fois, ce qui évite le problème des nombres – uniques – avec un espace.\n",
    "\n",
    "`‑trainingThreads 8` - Pour utiliser pleinement son processeur, indiquer le maximum selon le model de la documentation (On our 16-core test machines: a batch size of 10,000 runs fastest with around 6 threads; a batch size of 100,000 runs best with around 10 threads)\n",
    "\n",
    "`‑maxIter 5000` - Pour arrêter l’entraînement après 5'000 itérations, de base il fait 20'000 itérations.\n",
    "\n",
    "#### Test file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-test.conllu3 -model data/UD_French_train.gz`\n",
    "\n",
    "Loading depparse model: data/UD_French_train.gz ...\n",
    "###################\n",
    "#Transitions: 91\n",
    "#Labels: 45\n",
    "\n",
    "ROOTLABEL: root\n",
    "\n",
    "PreComputed 100000, Elapsed Time: 1.226 (s)\n",
    "\n",
    "Initializing dependency parser ... done [3.0 sec].\n",
    "\n",
    "Test File: data/fr-ud-test.conllu3\n",
    "\n",
    "OOV Words: 1100 / 10020 = 10.98%\n",
    "\n",
    "UAS = 77.8842\n",
    "\n",
    "LAS = 71.4571\n",
    "\n",
    "DependencyParser parsed 10020 words in 416 sentences in 1.4s at 7308.5 w/s, 303.4 sent/s.\n",
    "\n",
    "\n",
    "#### Dev file\n",
    "##### Dans la ligne de commande:\n",
    "`java -mx2000m -cp stanford-corenlp-3.9.2.jar edu.stanford.nlp.parser.nndep.DependencyParser -t\n",
    "estFile data/fr-ud-dev.conllu3 -model data/UD_French_train.gz`\n",
    "\n",
    "Loading depparse model: data/UD_French_train.gz ...\n",
    "###################\n",
    "#Transitions: 91\n",
    "#Labels: 45\n",
    "\n",
    "ROOTLABEL: root\n",
    "\n",
    "PreComputed 100000, Elapsed Time: 1.497 (s)\n",
    "\n",
    "Initializing dependency parser ... done [2.7 sec].\n",
    "\n",
    "Test File: data/fr-ud-dev.conllu3\n",
    "\n",
    "OOV Words: 4724 / 35771 = 13.21%\n",
    "\n",
    "UAS = 80.5094\n",
    "\n",
    "LAS = 74.5241\n",
    "\n",
    "DependencyParser parsed 35771 words in 1478 sentences in 2.7s at 13165.6 w/s, 544.0 sent/s.\n",
    "\n",
    "### Question\n",
    "Quel est le score du modèle fourni, et quel est le score du modèle que vous avez entraîné ?\n",
    "\n",
    "## Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "from collections import Counter\n",
    "from nltk.parse import (\n",
    "    DependencyGraph,\n",
    "    ProjectiveDependencyParser,\n",
    "    NonprojectiveDependencyParser,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vous devez lire le(s) fichier(s) UD phrase par phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/fr-ud-test.conllu3'  \n",
    "dependency_graphs = []\n",
    "\n",
    "try:  \n",
    "    fp = open(filepath, 'r',encoding=\"utf-8\")\n",
    "    raw_sentences = fp.read().split('\\n\\n')\n",
    "finally:  \n",
    "    fp.close()\n",
    "\n",
    "nbr_raw_sents = len(raw_sentences) - 1 ## the file end with 4 '\\n'\n",
    "    \n",
    "for i in range(nbr_raw_sents):\n",
    "    try:\n",
    "        dependency_graphs.append(DependencyGraph(raw_sentences[i], top_relation_label='root'))\n",
    "    except:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il faut ensuite extraire les triplets ayant une relation ‘nsubj’ (entre sujet et verbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubj_triples = []\n",
    "\n",
    "for dependency_graph in dependency_graphs:\n",
    "    for head, rel, dep in dependency_graph.triples():\n",
    "        if rel == 'nsubj':\n",
    "            nsubj_triples.append((head, dep))\n",
    "            \n",
    "occurences = Counter(nsubj_triples)\n",
    "\n",
    "# sorted_by_second = sorted(list(occurences.items()), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quels sont les 10 triplets les plus fréquents dans tout le corpus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Il a\" occure 7 fois \n",
      "\"on peut\" occure 4 fois \n",
      "\"il a\" occure 4 fois \n",
      "\"c' est\" occure 3 fois \n",
      "\"il contrôle\" occure 3 fois \n",
      "\"il faut\" occure 3 fois \n",
      "\"qui font\" occure 2 fois \n",
      "\"elle guette\" occure 2 fois \n",
      "\"vous avez\" occure 2 fois \n",
      "\"je vois\" occure 2 fois \n"
     ]
    }
   ],
   "source": [
    "# sorted_by_second[:10]\n",
    "occurences.most_common(10)\n",
    "\n",
    "for (triplet, nbr_occurences) in occurences.most_common(10):\n",
    "    print('\"{pron} {verb}\" occure {nbr_occurences} fois '.format(\n",
    "        pron=triplet[1][0],\n",
    "        verb=triplet[0][0],\n",
    "        nbr_occurences=nbr_occurences,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des arbres syntaxiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Génération depuis le fichier .conllu en passant par le serveur CoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Démarrer le serveur, timeout d'une demie heure pour la génération de 412 arbres syntaxiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-french.properties -port 9000 -timeout 1800000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.tree import Tree\n",
    "\n",
    "import os, codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection au serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  ROOT                                         \n",
      "                                   |                                            \n",
      "                                  SENT                                         \n",
      "                                   |                                            \n",
      "                                   NP                                          \n",
      "  _________________________________|___________________________________         \n",
      " |     |                 PP                              |             |       \n",
      " |     |       __________|_____                          |             |        \n",
      " |     |      |                NP                        |             |       \n",
      " |     |      |     ___________|___                      |             |        \n",
      " |     |      |    |               PP                    |            AdP      \n",
      " |     |      |    |      _________|____                 |         ____|____    \n",
      " |     |      |    |     |              NP               |      MWADV       |  \n",
      " |     |      |    |     |          ____|_______         |    ____|____     |   \n",
      "PRON  VERB   VERB CONJ  VERB      DET          NOUN     ADP DET       ADV  ADV \n",
      " |     |      |    |     |         |            |        |   |         |    |   \n",
      "Nous aimons faire  et  rendre     nos      laboratoires  à   le       plus vite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next(parser.raw_parse('Nous aimons faire et rendre nos laboratoires au plus vite')).pretty_print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parsing du fichier .conllu pour récupérer les phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source : http://techstuffbrazil.blogspot.com/2017/03/quick-tutorial-to-nltk-corpus-reader-of.html\n",
    "#         labo2\n",
    "\n",
    "root = './data/'\n",
    "test = 'fr-ud-test.conllu3'\n",
    "COLUMN_TYPES = ('ignore', \n",
    "                'words', \n",
    "                'ignore', \n",
    "                'pos', \n",
    "                'ignore', \n",
    "                'ignore', \n",
    "                'tree', \n",
    "                'ignore', \n",
    "                'ignore', \n",
    "                'ignore')\n",
    "\n",
    "testFile  = ConllCorpusReader(root=root, \n",
    "                                  fileids=test, \n",
    "                                  columntypes=COLUMN_TYPES, \n",
    "                                  encoding='utf8', \n",
    "                                  separator=\"\\t\", \n",
    "                                  tagset='universal')\n",
    "\n",
    "#Nous voulons un tableau contenant toutes les phrases du fichier .conllu\n",
    "test_sentences = testFile.sents()\n",
    "\n",
    "# TODO rendre ça plus propre\n",
    "sentences = []\n",
    "for sentence in test_sentences:\n",
    "    current_sentence = ''\n",
    "    for word in sentence:\n",
    "        current_sentence = current_sentence + word + ' '\n",
    "    sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création d'un tableau avec tous les arbres représentants les phrases, ainsi qu'un fichier contenant les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini avec 412 arbres syntaxiques\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "\n",
    "filename2 = \"sentences_trees.txt\"\n",
    "\n",
    "if os.path.exists(filename2): # Si le fichier existe, pas besoin de tout générer à nouveau\n",
    "\n",
    "    f = open(filename2, \"r\", encoding='utf8')\n",
    "\n",
    "    try:\n",
    "        for line in f:\n",
    "            trees.append(Tree.fromstring(line)) # On génère les arbres syntaxique\n",
    "    finally:\n",
    "        f.close()\n",
    "\n",
    "else: # Si le fichier n'existe pas, on génère les arbres et on remplit le fichier\n",
    "    fd = codecs.open(filename2, 'a', 'utf8')\n",
    "\n",
    "    # Aide pour savoir à quand en est la génération\n",
    "    index = 0\n",
    "    length = len(sentences)\n",
    "    \n",
    "    # chaque phrase est transformée en arbre syntaxique qui est stocké dans un tableau ainsi qu'un fichier\n",
    "    for sentence in sentences:\n",
    "        index = index + 1\n",
    "        try:\n",
    "            tree = next(parser.raw_parse(sentence)) # Génération de l'arbre syntaxique\n",
    "            trees.append(tree)\n",
    "            fd.write(tree._pformat_flat(nodesep='', parens='()', quotes=False) + \"\\r\\n\")\n",
    "        except:\n",
    "            print('Error')\n",
    "        if index % 1 == 0:\n",
    "            print(index/length*100)\n",
    "\n",
    "    fd.close()\n",
    "\n",
    "print('fini avec {} arbres syntaxiques'.format(len(trees)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction de TOUS les groupes nominaux(NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = []\n",
    "\n",
    "for tree in trees: # Pour chaque arbre\n",
    "        for s in tree.subtrees(lambda t: t.label() == 'NP'): # Nous prenons tous ses sous-arbres NP\n",
    "            str_now = s.flatten()\n",
    "            test = str_now.__str__().replace('\\n', '').replace('  ', ' ')\n",
    "            nps.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indication des 10 NP les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(NP le à)', 24),\n",
       " ('(NP le)', 11),\n",
       " ('(NP )', 8),\n",
       " ('(NP les à)', 6),\n",
       " ('(NP le Sahara occidental)', 4),\n",
       " ('(NP ,)', 4),\n",
       " ('(NP le pays)', 4),\n",
       " ('(NP le Maroc)', 4),\n",
       " ('(NP Il est)', 3),\n",
       " ('(NP le à le)', 3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurences = Counter(nps)\n",
    "occurences.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
